# ğŸ“ Next Word Predictor (LSTM-based)

A deep learning project that predicts the next word in a sequence using an **LSTM (Long Short-Term Memory)** model.  
This project demonstrates how recurrent neural networks can learn language context and improve predictive text generation.

---

## ğŸš€ Overview

The **Next Word Predictor** leverages an **LSTM-based neural network** to model sequential dependencies in text data.  
It takes a sequence of words as input and predicts the **most likely next word**, making it suitable for applications such as:

- Predictive typing (like in keyboards)  
- Smart text editors  
- Chatbots & conversational AI  
- Writing assistance tools  

---

## âœ¨ Features

- LSTM-based sequence modeling for language prediction.  
- Preprocessing with tokenization and sequence padding.  
- Trainable on any custom text dataset.  
- Real-time next word prediction with trained weights.  
- Simple and modular code structure for easy experimentation.  

---

## âš™ï¸ Installation

1. Clone the repository:
```bash
git clone https://github.com/AshutoshSingh840/Next_Word_Predictor.git
cd Next_Word_Predictor
```
2. Install dependencies:

```bash
pip install -r requirements.txt
```
# ğŸ› ï¸ Usage
Training the Model
Prepare your dataset (plain text file, one sentence per line).
Run:
```bash
python train.py --data data/corpus.txt --save-path model.h5
```

Predicting Next Word
```python```
from model import NextWordPredictor

Load the trained LSTM model
predictor = NextWordPredictor(model_path="model.h5", tokenizer_path="tokenizer.pkl")

Predict the next word
input_text = "The quick brown"
print("Prediction:", predictor.predict_next(input_text))

# ğŸ“Š Model Details

Architecture:

  â€¢ Embedding layer
  â€¢ LSTM layers (1â€“2 stacked)
  â€¢ Dense + Softmax output layer
  â€¢ Loss Function: Categorical Crossentropy
  â€¢ Optimizer: Adam
  â€¢ Metrics: Accuracy

# ğŸ“ˆ Results
 â€¢ Training and evaluation results of the LSTM-based Next Word Predictor:
 â€¢ Model improves steadily over 50 epochs.
 â€¢ Loss decreases while accuracy increases, showing the modelâ€™s ability to learn sequential dependencies.

# ğŸ¤ Contribution
Contributions are welcome!
Follow these steps:

â€¢ Fork the repo
â€¢ Create a branch (git checkout -b feature-name)
â€¢ Commit changes (git commit -m "Add feature")
â€¢ Push branch (git push origin feature-name)
â€¢ Create a Pull Request

ğŸ“œ License
This project is licensed under the MIT License. See the LICENSE file for details.

